README

Dates are YYYY MM DD.

2018 09 26
	Fixed organization of files and added __init__.py where sensible.
	Fixed really strange import mishaps with pylint and package organization.
	Added in base abstract classes for AgentSystem and Domain and file stubs.
	Added in some typing hinting here and there. (Look at AgentSystem for type hinting examples.)
	See commit history for files modified.

2018 09 28
    Finished up domain.
    Added in abstract and initial AgentSystem properties.
	See commit history for files modified.

2018 10 03 - 2018 10 05
    Since it would make more sense to deal with configs and properties once we have some concrete subclasses going,
    I started earlier on IndependentSystem and SharedSystem. I also worked on the properties and config as planned.
    I can't think of any configuration

    There was some TODOs for documentation which I filled out.

    I implemented some of the config auto-loading. We will have to verify this very soon.

    I also refactored Domain so it uses DomainItem instead of some awful hackneyed constructor.
    Work with Domain otherwise was unexpectedly low in quantity.
    This is also on account of ActionDomain and ObservationDomain so far not being demonstrably significant.

    DISCUSSED
    Things we need to discuss again: Agent ID? It would be bloated to have agent class as a wrapper dict layer, but
        is it ok to just pass in canonical agent IDs as Dict[agent_class: int, agent_ids: List[int]]? from env to asys?

2018 10 12
    SUMMARY:
    A rather bad week, all things considered. It was good that I saved some work with the last week.
    We decided to use canonical agent IDs. I am having it implied as agent_class_map through the asys init.

    Implemented most of the rest for the independent systems, finishing up expedited work from last week.
    Now we're pretty much even.

    Have some trouble with verification currently because of lack of integration from environment.
    To that end, I did some code cleanup while making sure things were interconnected in expected ways.
    Also added some stubs for next week, for testing, and added some module-level (exports?) imports.

    Common:
        Implemented equality for DomainItem, Domain, feature comparison and equality for AgentsDomain.
        Added a DomainTransferMessage class for disambiguating differences in domain transfer.

    Made proposals for a lot of things. Now we also have a giant (but centralized) to do list for discussion.

    ===

    DETAILS:
    While working on learn_update, I realized something rather awful about how Trajectories are handled.
        Since we (foolishly, in hindsight) pack all agents' data per agentclass into arrays, we have to:
            Allocate and write to [N C] array observations in env update
                (relatively little time savings, and also frustrating for the programmer)
            Split it and recombine into appropriate combinations for the policy groups in asys get_actions
                (wastes time and adds frustration)
            List into trajectory in Controller
            Split from EACH TIMESTEP due to bad ordering, from Controller, for update step
                (huge O(n) time waste and a ton of frustration)
    An alternative:
        Observation: List[np.ndarray] is not by agent class, but rather by canonical agent ID. Then:
            Allocate and write to N [C] vectors in env update
            Combine into appropriate combinations in asys get_actions
            List into trajectory per canonical agent ID in Controller
            Easily separable for algorithm trajectories by "only" stacking at each timestep

    In general though, we still run into the problem that we have to do the combination step.
        If we are able to record the trajectories IN the correct policy-group formations that asys expects,
        we can avoid these issues almost entirely.
        But that results in the problem of recorded trajectories possibly being problematic to interpret...
        Then again, maybe we can have both. It'd be up to Controller.

    For CoordinationGraphQLearning we definitely have to consider it carefully.
        Each PG should be a separated subgraph. They shouldn't talk to each other.

2018 10 14
    Discussed with Micheal about interplay between alternate action sampling methods for multiagent systems.
    We decided that AgentSystem should handle the EGreedyWithRestarts portion of Coordination Graph Q Learning...
        Alternative methods of extracting group actions from the edge Q values might be switched via config.
            I might want to consider having a GroupSampler under asys to handle it if it grows beyond expected.

    We also finalized some topics about changes in Policy and Algorithms and how Domain should work with canonical agent
        IDs and also methods for team collaboration on Properties objects.

    Discussed:
        - Why was the Policy.__init__ changed?
        - Algorithm Property for simultaneous update


2018 10 16
    Discussed with Sibi about Environment. Cleared up some mystery about how to use Domain and some questions of accel.

    I will also be creating an environment before thursday.

    Discussed:
        - Inform others of agent_class_map for canonical Agent ID
        - Workaround to eliminate the requirement of well-ordering for agent id in agent class

2018 10 18
    Did some work here and there on graph asys and figuring out EGreedyWithRestarts for continuous action spaces.

2018 10 19
    Implement Domain join.
    Add in a text file for requested changes from other files.

    "Finished up" CoordinationGraphSystem and found out about a ton of awful, horrible issues I didn't think about.

    Greedy with restarts for Graph ASYS.

    Started to impl max-plus and agent elim but because of the massive amount of impending questions and decisions
        left it out of this commit.

    DONE - NEED to be able to see Q from policy? Also - are we regretting allowing the action selection methods
        from coordination graph system into the asys? need to discuss AGAIN about including in policy
        is CoordGraph learning intrinsically tied to Q-learning? Probably.
        Feel like the calls of policy.get_q should be directed to the value approximator instead.
    DONE - NEED to talk to ayush IMMEDIATELY about passing of trajectories and datapoints between env, asys, thru ctrlr.
    DONE determine behavior of random sampling on open action domains (not supporting this.)

2018 10 25
    currently making example configs
    new "Big Properties" implementation for single centralized checklist and config

    discussed modifications to program flow, taking care of joint observations, trajectory management, immediate work
        direction, and config setup and structure and how it should be taken care of.

2018 10 26
    Need to meet with entire group for validation. It turned out to be much more of hassle than previously expected.
    Wrote up Max_Plus. Attempted impl of Agent-Elimination but found out that it doesn't play nice and wasn't apparently
        working in the original IRL framework. Lowered its priority.

    Started on resource collection environment. Will use this for some initial testing (of more complicated environment)

    We discussed the future of ObservationDomain and ActionDomain use in environment wrt. "using slicers"
    Creating a metaprogramming framework is probably a fast track to insanity. We decided to leave it to comments
        instead of complicating lives with slicer getting.

2018 11 2
    Focused this week mostly on the essay.

    Created some appropriate configs for asys and some environments.
    Created example entrypoint config.
    Updated configs to use "name" correctly.
    Updated config to load using new method.

    More work on ResourceCollectionEnvironment.

    Config loading for Policy and Algorithm is still put on hold until this weekend when we can better coordinate.
    Entrypoint is put on hold because we definitely need to coordinate well for that.

    The gantt chart that I had planned is probably better off scrapped in this regard - it is rather silly and filled
        with unfounded, unresearched experimental content... We are better off trying to get working things first.
        But my work this week is slightly stoppered by absent coordination.... the essay timing was really unfortunate.

2018 11 3
    A ton of work! Bugfixes everywhere, refactoring, setting up importlib correctly (nightmarish).
    A lot more cohesive work was done today compared to a lot else...

    We are finally doing run-test driven cycles.
    The naming convention has also been altered, but for now, the next step will come when discrete_max.json is there.

2018 11 04
    Alternative controller for impatient testing.
    Proposed splitting up env update to update and observe.

2018 11 05
    Found out python version requirement (3.6.3).

    Edited policy to reflect the very nice fact that it /already contains domain_act/ when it is initialized.
    Policy shouldn't take in anything other than observation?

    Numerous bugfixes for initial running.
    Including throughout Policy and with TestEnvironment, which was surprising.

    NOTES: Availability shouldn't exist; Invalid actions should be ignored.
        e.g. since robots should always be able to go forward, even if they could bump into walls
        Being able to write constraints is basically hard-coding decisions.

        Observability can be special DomainItems in ObservationDomain.

    It lives! debug.json runs the main loop over mdp_controller.

2018 11 09
    Most of my work was from 3-4-5 this week.

    note: gridworld agents shouldn't be able to occupy the same spot.

2018 11 12
    Refactors, prep for openai predator prey, resource collection environment notes.

2018 11 16
    Joint observation code and support in controller.

    Fixing up CoordinationGraphSystem with revised joint observation handling.

    OpenAIPredatorPrey got stalled by hardware issues over these days...

    New use_discrete property.
    New transfer_method config for CoordinationGraphSystem.
    drop, map to neighbors, project over existing implemented.

    CGS now only supports discrete action domains.

    NOTE: CGS uses switching on single agent actions... what about joint actions? Switch on combinations?
    Revise the algorithm once it is being tested...
    DONE also consider the issue with predatorpreybasis being a very... hardcode-heavy... functionapproximator.

2018 11 28

    Modifications to agentsystem to update it for the new Algo+Policy configuration.

2018 11 29

    Introduction of Model and a nice return to the stateless algorithm we all wanted. Numerous small changes.
    Fix to CGQL domain transfer to work with new all-tensorflow code.
    Some small work on PredatorPrey and decision to implement here instead of using third-party non-openAI code.
    Prelim work for CGQL dict chaining reduction.

    Support for required properties fields for experiments (see Properties)

    DONE? bring up benchmarking via callback (just use @profile and kernprof)
    DONE reminder on gridworld env. to have config function
    DONE dict map chaining in CGS is quickly becoming insane. preserve caching by creating a caching object. fix it.

    DONE reminder to self. refactor <Domain>.join to a proper class method...
        if we join multiple things, it is hideously inefficient right now

2018 12 1

    New, nicer, refactored PolicyGroup
    More PredatorPrey work.

2018 12 2

    More CGSys work.
    More PredatorPrey work.
    Some discussion about custom Q-Func basis functions...

2018 12 3

    Special Basis is just a way of obtaining transforming observation and action before they are pushed into the traj
    Made DiscreteIndexedActionDomain and appropriate and efficient (hastily benchmarked) composition and decomposition.

2018 12 5

    function_approximator:
        ActionFreeLinear (consider as "extensibility test") with decent immediate success.
        DONE to discuss: elig. trace is not clear on how to implement...!
    common:
        DiscreteActionFeatureDomain and weird python class nonsense for static join method hierarchy correctness.
    env:
        PredPrey has random option for pred/prey slots.
        Predatorprey feature domains and joint action.
        enforced repeat+tile for joint feature creation.
    asys:
        Numerous bugfixes in CoordinationGraphSystem
    controller:
        DONE Prepare to call resets at beginning of episodes
    configs:
        Configs for ActionFreeLinear
        Configs for PredPrey

    current commit is intermediate - everything up to predprey observe (data)
    DONE? episode end w/ calls to policies? models? thru asys
    DONE saving function approximators...

2018 12 6
    PredatorPrey domain code and single agent observe & bugfixes.

2018 12 7
    PredatorPrey single agent observe and joint agent observe & numerous bugfixes.
    PredPrey isolated test
    DONE Call env resets at beginning of episodes. (do asys need to be reset?)
    Callback CALLS but no CALLBACKS...

2018 12 08
    Action selection completely refactored. It turned out pretty nice, too!
    Implemented agent elimination.
    Verified max_plus
    Get/set weights for ActionFreeLinear.
    Numerous bugfixes
    Set up config to load many moduleconfigs for each callback. (!!!)
    End of episode stuff
    Changed a bit of how policy groups are organized in agent system.
    Verified greedy_with_restarts and agent_elimination too, in a unit test!

2018 12 09
    Changed gridworld observation to be better to learn on (relative features)
    Made Drift environment, which is a simpler one than cartpole.
    Merged ayush's code into master branch.
    Lots of gridworld fixes! (Again...! Absolute features.)
    Lots of refactoring of controller.
    Designed and wrote reward eval cycle code for controller and refactored into callbacks.

    Redesigned callback by removing unnecessary public/private wrapping.
    Make CallbackImpl for callback implementations' detection.

    Remove BaseLogger. Obsoleted given that MDPController needs trajectory tracking anyway.
    Remove callbacks.Statistics because it was clearly wrong: Hardcoded for the TestEnvironment (!?)
        and didn't account for Trajectory actually... already having the rewards......
        compute_performance was removed because that requires actual evaluation.
            Just looking at the performance of the trajectory itself is unreliable, since
            it might have been constantly updated by an online algorithm.
            As well, the calculation of gamma is something that should be left to actual algorithms.
            (Which, we can't actually cache, since we constantly re-evaluate past timesteps using the current policy.)

    Implement horrifying, cursed Callback Loader Hook in controller
    Implement visualizer callback. Some bugs exist. Current function of closing viz window is to terminate episode...
        (Will people misuse this for learning episodes? It worries me.)
    Implement cumulative reward callback (by refactoring from controller).
    Implement reward plot callback (by refactoring from controller).

    Fixed up ActionFreeLinear's elig trace learning stuff.... probably. It makes SARSAOnline run better on cartpole too.

    Fixed bugs with PredatorPrey which somehow got past the radar (zombie prey, teleporting predators)
    CGQL+PredPrey with max_plus learns!
    Debugged agent_elimination (even though it runs, make sure it's correct)
    Runs with agent_elimination and captures prey. It's just unclear whether or not this is learning properly...?
    Also runs with greedy_with_restarts.
    But, maybe it's not really giving it credit for not immediately doing well in 10 episodes.
    These are kind of slow. It'd be nice to benchmark.

    Controller now indexes episodes from 1.

    Trained for 100 episodes w/ agent elim, lambda 0.3, LR 1e-5. Weights exploded.
        It's really weird that it continued to work after the weights went bad. (?!)
        For some episodes afterward we could occasionally catch all prey.
        Looking at the plot, it's clear it trained. I guess prey randomness is rather bothersome? We'll see.

        Also, Evaluate makes no sense for CGQL since it bypasses the policy sampler.

2018 12 10-11 (overnight)

    Visualize predatorprey effectively.
    Fix docstrings and documentation around in general.
    Made nice animations and slides for presentation.
    Add SaveBestEvaluation callback
    Add LoadPolicy callback
    Add asys.save, asys.load (hacky)
    Add SaveTrajectories callback

    DONE benchmark CGQL to find out what is slow
    DONE verify asys save/load
    DONE go over edit of TDOnline and also try CGQL+PredPrey on longer episodes
    DONE (extend) expose ConfigItem from config_signature instead of lambdas w checks
    DONE (optimize) structured array for trajectory instead of namedtuple (sort of)

    TODO verify savetrajs clb
    TODO prohibit loading wrong-shaped policies / cross-FA data into agentsystem

    TODO (prefer) separate class loading from config loading to support script runner
    TODO (future) script runner

    TODO (prefer) verify ResourceCollection
    TODO (extend) gui runner with smart warnings / prohibitions

    TODO (future) nicer-looking docs
    TODO (future) smart callbacks to avoid trusting user for impl_dict

    TODO (future) drange for multidimensional domainItem (...!?)

===========================
